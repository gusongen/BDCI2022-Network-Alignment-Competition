{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5e7ecd-dc40-406c-921a-81f1609e2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device is cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import sklearn\n",
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import argparse\n",
    "np.set_printoptions(suppress=True)\n",
    "# tensorboard\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.loss import customized_loss, margin_ranking_loss, custom_loss_trusted_err\n",
    "from src.dataset import Dataset\n",
    "from src.layers import GraphConvLayer\n",
    "from src.utils import generate_neg_sample, load_data\n",
    "from src.model import Model\n",
    "import os.path\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"current device is {device}\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "parser.add_argument(\"--epoch\", type=int, default=10, help=\"epoch to run\")\n",
    "parser.add_argument(\"--seed\", type=int, default=8, help=\"training set ratio\")\n",
    "parser.add_argument('--hidden', type=int, default=128, help=\"hidden dimension of entity embeddings\")\n",
    "parser.add_argument('--lr', type=float, default=0.01, help=\"learning rate\")\n",
    "parser.add_argument('--k', type=float, default=10, help=\"hit@k\")\n",
    "parser.add_argument('--negsize', type=int, default=10, help=\"number of negative samples\")\n",
    "parser.add_argument('--negiter', type=int, default=10, help=\"re-calculate epoch of negative samples\")\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5, help=\"weight decay coefficient\")\n",
    "parser.add_argument('--graph_s', type=str, default=\"data_G1\", help=\"source graph path\")\n",
    "parser.add_argument('--graph_d', type=str, default=\"data_G2\", help=\"destination graph path\")\n",
    "parser.add_argument('--anoise', type=float, default=0.2, help=\"anchor noise\")\n",
    "parser.add_argument('--board_path', type=str, default='board', help=\"tensorboard path\")\n",
    "\n",
    "args,unknow =parser.parse_known_args()\n",
    "# args = parser.parse_args()\n",
    "# args = parser.parse_known_args()[0]\n",
    "# args,unknow =parser.parse_known_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1445a03-9ef1-4ac2-9992-9a7a8db4324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#total params: 323584\n",
      "training samples: 363, test samples: 91\n",
      "model architecture:\n",
      " Model(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (gcnblocks): ModuleList(\n",
      "    (0): GraphConvLayer(\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (1): GraphConvLayer(\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(90, 2) (90, 2)\n",
      "epoch: 1, loss: 27735.174\n",
      "\n",
      "current best Hits@1 count at the 2th epoch: 1\n",
      "epoch: 2, loss: 32104.65\n",
      "\n",
      "epoch: 3, loss: 21750.719\n",
      "\n",
      "epoch: 4, loss: 16277.494\n",
      "\n",
      "epoch: 5, loss: 17253.939\n",
      "\n",
      "epoch: 6, loss: 14943.545\n",
      "\n",
      "epoch: 7, loss: 10047.324\n",
      "\n",
      "epoch: 8, loss: 7851.937\n",
      "\n",
      "epoch: 9, loss: 6210.887\n",
      "\n",
      "epoch: 10, loss: 5608.578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# parameters\n",
    "epoch = args.epoch\n",
    "embedding_dim = args.hidden\n",
    "learning_rate = args.lr\n",
    "weight_decay = args.weight_decay\n",
    "neg_samples_size = args.negsize\n",
    "negiter = args.negiter\n",
    "graph_path_s = args.graph_s\n",
    "graph_path_d = args.graph_d\n",
    "train_seeds_ratio = args.seed * 0.1\n",
    "k = args.k\n",
    "anoise = args.anoise\n",
    "############################\n",
    "tb_logger = SummaryWriter(args.board_path)\n",
    "\n",
    "############################\n",
    "# preprocess\n",
    "graph1 = graph_path_s\n",
    "graph2 = graph_path_d\n",
    "A1, A2, anchor = load_data(graph1=graph1, graph2=graph2, anoise=anoise)\n",
    "train_size = int(train_seeds_ratio * len(anchor[:, 0]))\n",
    "test_size = len(anchor[:, 0]) - train_size\n",
    "# train_set, test_set = torch.utils.data.random_split(anchor, lengths=[train_size, test_size], \n",
    "                                                        # generator=torch.Generator().manual_seed(43))\n",
    "train_set, test_set = torch.utils.data.random_split(anchor, lengths=[train_size, test_size])\n",
    "train_set = np.array(list(train_set))\n",
    "test_set = np.array(list(test_set))\n",
    "batchsize = train_size\n",
    "train_dataset = Dataset(train_set)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=False)\n",
    "model = Model(Variable(torch.from_numpy(A1).float()), Variable(torch.from_numpy(A2).float()), embedding_dim=embedding_dim)\n",
    "optimizer = torch.optim.Adagrad(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.TripletMarginLoss(margin=3, p=2)\n",
    "############################\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'#total params: {pytorch_total_params}')\n",
    "print(f\"training samples: {train_size}, test samples: {test_size}\")\n",
    "print(f\"model architecture:\\n {model}\")\n",
    "\n",
    "\n",
    "def predict(output_file, sim_measure=\"cosine\",):\n",
    "    \"\"\"\n",
    "    将预测写入文件\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Embedding1, Embedding2 = model()\n",
    "    Embedding1 = Embedding1.detach()\n",
    "    Embedding2 = Embedding2.detach()\n",
    "\n",
    "    # step 1: generate sim mat\n",
    "    if sim_measure == \"cosine\":\n",
    "        # similarity_matrix = cosine_similarity(Embedding1, Embedding2)\n",
    "        similarity_matrix = torch.mm(Embedding1, Embedding2.t()).cpu().numpy()\n",
    "    else:\n",
    "        Embedding1 = Embedding1.numpy()\n",
    "        Embedding2 = Embedding2.numpy()\n",
    "        similarity_matrix = euclidean_distances(Embedding1, Embedding2)\n",
    "        similarity_matrix = np.exp(-similarity_matrix)\n",
    "    \n",
    "    if (os.path.isfile(output_file)==False):\n",
    "        os.mknod(output_file)\n",
    "    \n",
    "    # step 2: information statistics\n",
    "    # alignment_hit1 = list()\n",
    "    file = open(output_file, 'w')\n",
    "    for idx0, line in enumerate(similarity_matrix):\n",
    "        idx = np.argmax(line)\n",
    "        idx = int(idx)\n",
    "        # alignment_hit1.append(idx)\n",
    "        file.write(f'{idx0} {idx}\\n')\n",
    "\n",
    "\n",
    "def evaluate(data, k, sim_measure=\"cosine\", phase=\"test\"):\n",
    "    model.eval()\n",
    "    Embedding1, Embedding2 = model()\n",
    "    Embedding1 = Embedding1.detach()\n",
    "    Embedding2 = Embedding2.detach()\n",
    "    if phase == \"over\":\n",
    "        print(Embedding1)\n",
    "    # step 1: generate sim mat\n",
    "    if sim_measure == \"cosine\":\n",
    "        # similarity_matrix = cosine_similarity(Embedding1, Embedding2)\n",
    "        similarity_matrix = torch.mm(Embedding1, Embedding2.t()).cpu().numpy()\n",
    "    else:\n",
    "        Embedding1 = Embedding1.numpy()\n",
    "        Embedding2 = Embedding2.numpy()\n",
    "        similarity_matrix = euclidean_distances(Embedding1, Embedding2)\n",
    "        similarity_matrix = np.exp(-similarity_matrix)\n",
    "    # step 2: information statistics\n",
    "    alignment_hit1 = list()\n",
    "    alignment_hitk = list()\n",
    "    for line in similarity_matrix:\n",
    "        idx = np.argmax(line)\n",
    "        idx = int(idx)\n",
    "        alignment_hit1.append(idx)\n",
    "        idxs = heapq.nlargest(k, range(len(line)), line.take)\n",
    "        alignment_hitk.append(idxs)\n",
    "    # step 3: calculate evaluate score: hit@1 and hit@k\n",
    "    hit_1_score = 0\n",
    "    hit_k_score = 0\n",
    "    for idx in range(len(data)):\n",
    "        gt = data[idx][1]\n",
    "        if int(gt) == alignment_hit1[idx]:\n",
    "            hit_1_score += 1\n",
    "        if int(gt) in alignment_hitk[idx]:\n",
    "            hit_k_score += 1\n",
    "    return similarity_matrix, alignment_hit1, alignment_hitk, hit_1_score, hit_k_score\n",
    "\n",
    "\n",
    "# begin training\n",
    "best_E1 = None\n",
    "best_E2 = None\n",
    "best_hit_1_score = 0\n",
    "neg1_left, neg1_right, neg2_left, neg2_right = generate_neg_sample(train_set, neg_samples_size=neg_samples_size)\n",
    "\n",
    "trusted_pair = np.loadtxt(f'data/trusted_pair.txt', delimiter=' ')\n",
    "err_pair = np.loadtxt(f'data/err_pair.txt', delimiter=' ') #(90,2)\n",
    "print(trusted_pair.shape, err_pair.shape)\n",
    "\n",
    "trusted_left, trusted_right = trusted_pair[:,0], trusted_pair[:,1] # (90,) (90,)\n",
    "err_left, err_right =  err_pair[:,0], err_pair[:, 1] # (90,) (90,)\n",
    "                                       \n",
    "for e in range(epoch):\n",
    "    model.train()\n",
    "    if e % negiter == 0:\n",
    "        neg1_left, neg1_right, neg2_left, neg2_right = generate_neg_sample(train_set, neg_samples_size=neg_samples_size)\n",
    "    for _, data in enumerate(train_loader):\n",
    "        a1_align, a2_align = data\n",
    "        E1, E2 = model()\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"---\")\n",
    "        # print(E1.shape, E2.shape)             #torch.Size([1135, 128]) torch.Size([1135, 128])\n",
    "        # print(a1_align.shape, a2_align.shape) #torch.Size([363]) torch.Size([363])\n",
    "        # print(neg1_left.shape, neg1_right.shape)       #(3630,) (3630,)\n",
    "        # print(\"-*-\")\n",
    "        \n",
    "        # loss = customized_loss(E1, E2, a1_align, a2_align, neg1_left, neg1_right, neg2_left, neg2_right, neg_samples_size=neg_samples_size, neg_param=0.3)\n",
    "        loss = custom_loss_trusted_err(err_left, err_right,\n",
    "                                       trusted_left, trusted_right,\n",
    "                                       E1, E2, \n",
    "                                       a1_align, a2_align, \n",
    "                                       neg1_left, neg1_right, \n",
    "                                       neg2_left, neg2_right, \n",
    "                                       neg_samples_size=neg_samples_size, \n",
    "                                       neg_param=0.3) # neg_param=0.3 没有使用 \n",
    "        # loss = margin_ranking_loss(criterion, E1, E2, a1_align, a2_align, neg1_left, neg1_right, neg2_left, neg2_right)\n",
    "        loss.backward()  # print([x.grad for x in optimizer.param_groups[0]['params']])\n",
    "        optimizer.step()\n",
    "        sim_mat, alignment_hit1, alignment_hitk, hit_1_score, hit_k_score = evaluate(data=test_set, k=k)\n",
    "\n",
    "        if hit_1_score > best_hit_1_score:\n",
    "            best_hit_1_score = hit_1_score\n",
    "            # todo save model\n",
    "            print(f\"current best Hits@1 count at the {e+1}th epoch: {best_hit_1_score}\")\n",
    "\n",
    "    tb_logger.add_scalar('loss_train', loss.item(), epoch)\n",
    "    print(f\"epoch: {e+1}, loss: {round(loss.item(), 3)}\\n\")\n",
    "\n",
    "# final evaluation and test\n",
    "# ground_truth = np.loadtxt('ground_truth.txt', delimiter=' ')\n",
    "# ground_truth = np.loadtxt('data/anchor/anchor_0.2_test.txt', delimiter=' ')\n",
    "# similarity_matrix, alignment_hit1, alignment_hitk, hit_1_score, hit_k_score = evaluate(data=ground_truth, k=k, phase=\"over\")\n",
    "# print(similarity_matrix)\n",
    "# print(f\"final score: hit@1: total {hit_1_score} and ratio {round(hit_1_score/len(ground_truth), 2)}, hit@{k}: total {hit_k_score} and ratio {round(hit_k_score/len(ground_truth), 2)}\")\n",
    "\n",
    "# 写入文件\n",
    "predict(f'submit_tmp_{args.graph_s}_{args.graph_d}_{anoise}.txt',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e682e-b903-46b8-bc32-ce8bb233afe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
